{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a49b0d4c05d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Intro to Spacy Basics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#Intro to Spacy Basics \n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "#if it was nlp = spacy.load(\"en_core_web_sm\") \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = (\"The patient has a hemoglobin of 24. No neurological changes. Cardiac improvement. Neuro changes still are at 0. Neurologically still sound. Neurologically the pupils are not dilated. Implementation of a chest tube. Seems extremely violent to feeding tubes and was in the ICU for 23 hours after a tumor removal from the brain in surgery. His white blood cell counts increased by 24%\")\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "#How to get noun phrases and verbs out of text\n",
    "#print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "nlp2 = English()\n",
    "first_token = text[0]\n",
    "\n",
    "print(first_token)\n",
    "tokens = []\n",
    "i = 0\n",
    "#x = true\n",
    "while(i < len(text)):\n",
    "    tokens.append(text[i])\n",
    "    #print(text[i])\n",
    "    i += 1\n",
    "    \n",
    "for token in doc:\n",
    "  \n",
    "    if token.like_num:\n",
    "        \n",
    "        next_token = doc[token.i + 1]\n",
    "        \n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)\n",
    "posdict = {\n",
    "    \n",
    "}\n",
    "for token in doc:\n",
    "    posdict[token.text] = token.pos_\n",
    "    #print(token.text, token.pos_)\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    i = 2\n",
    "\n",
    "\n",
    "#doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "pattern = [{'TEXT': 'Pupil'}, {'TEXT', 'Dilated'}]\n",
    "pattern2 = [{'TEXT': 'White blood cell counts'}, {'INCREASED', 'Increased'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Matcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7481868d747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Matches practice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#matcher.add('IPHONE_PATTERN', None, pattern)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#matcher.add('White blood cell', None, pattern2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Matcher' is not defined"
     ]
    }
   ],
   "source": [
    "#Matches practice\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "#matcher.add('White blood cell', None, pattern2)\n",
    "#matches = matcher(doc)\n",
    "#for match_id, start, end in matches:\n",
    "#    matched_span = doc[start:end]\n",
    "\n",
    "#print(matched_span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'white'},\n",
    "    {'LOWER': 'blood'},\n",
    "    {'LOWER': 'cell'},\n",
    "    {'IS_PUNCT': True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4b204a010918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hash values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love coffee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hash value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coffee'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3197928453018144401\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#hash values\n",
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', nlp.vocab.strings['coffee'])\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-47225df5a8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#doc and lexeme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love coffee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlexeme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coffee'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexeme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexeme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexeme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#doc and lexeme\n",
    "doc = nlp.make_doc(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7315c732ca18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#doc2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#doc2 = Doc(nlp.vocab, words=words, spaces=spaces)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Hello'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'world'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#Docs and Span Intro\n",
    "#doc2 \n",
    "#doc2 = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "from spacy.tokens import Doc, Span\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "\n",
    "doc = Doc(nlp.vocab, words= words, spaces=spaces)\n",
    "\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "\n",
    "doc.ents = [span_with_label]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e61d88fa381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#similarity trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#nlp = spacy.load('en_core_web_md')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi, my name is Lil Wayne and I am a rapper who is 42 years old\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdoce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, I am Tyga and I'm an artist who is \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"TESTTT\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#similarity trials\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "doct = nlp(\"Hi, my name is Lil Wayne and I am a rapper who is 42 years old\")\n",
    "doce = nlp(\"Hello, I am Tyga and I'm an artist who is \")\n",
    "#print(\"TESTTT\")\n",
    "print(doct.similarity(doc))\n",
    "print(doct[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Matcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1bc9d2770689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#implementation of matcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'White'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'blood'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cell'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The patient has a hemoglobin of 24. No neurological changes. Cardiac improvement. Neuro changes still are at 0. Neurologically still sound. Neurologically the pupils are not dilated. Implementation of a chest tube. Seems extremely violent to feeding tubes and was in the ICU for 23 hours after a tumor removal from the brain in surgery. His white blood cell counts increased by 24%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Matcher' is not defined"
     ]
    }
   ],
   "source": [
    "#implementation of matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('White', None, [{'LOWER': 'blood'}, {'LOWER': 'cell'}])\n",
    "doc = nlp.make_doc(\"The patient has a hemoglobin of 24. No neurological changes. Cardiac improvement. Neuro changes still are at 0. Neurologically still sound. Neurologically the pupils are not dilated. Implementation of a chest tube. Seems extremely violent to feeding tubes and was in the ICU for 23 hours after a tumor removal from the brain in surgery. His white blood cell counts increased by 24%\")\n",
    "\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    \n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    \n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ecc93e9f1a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#PhraseMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"white blood cells\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#PhraseMatcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp.make_doc(\"white blood cells\")\n",
    "matcher.add('cells', None, pattern)\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    \n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-608b1029e973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(\"CHICKENS\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "#print(\"CHICKENS\")\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b6c473481fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mterm_exists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp3 = spacy.blank()\n",
    "term_exists = false\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b8fde9c00720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Summary abilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#Summary abilities\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "nlp = spacy.load('en')\n",
    "def readingtime(mytext):\n",
    "    total_words = len([ token.text for token in nlp(mytext)])\n",
    "    estimated_time = total_words/200.0\n",
    "    \n",
    "stopwords = list(STOP_WORDS)\n",
    "print(len(stopwords))\n",
    "docc = (\"A terrorist group rises from the shadows. An anarchist, the Joker, tries to spark chaos throughout all of Gotham. A terrorist, Bane, takes Gotham and its citizens hostage and enforces anarchy. These are some of the rivals and villains, Bruce Wayne, and his alter ego Batman, had to face in his life in Gotham. Batman Begins, The Dark Knight, and The Dark Knight Rises take a deep look into Bruce Wayne and his obsession with Gotham and Batman, and how that almost ruins him as a person. Through the tragic death of his first love, Rachel Dawes, and his retirement of the Batman title, Bruce Wayne comes to peace with his alter ego, and finally becomes his own best friend. The Joker’s murder of Bruce Wayne’s love interest is a key moment in Batman’s character arc, as Bruce realizes that he is slowly turning into his worst nightmare. In The Dark Knight, with the Joker as the main antagonist, the Joker provides an ultimatum to Bruce. He has Harvey Dent and Rachel Dawes in hostage. The Joker gives the locations of the two, but if one were saved earlier than the other, then the other person would be murdered. Batman, in his fury and quest to save Rachel, goes to the location where the Joker said Rachel was, but since the Joker had switched the locations, Bruce had found Harvey. He had not communicated with the police, so by the time they had reached Rachel, she had perished. Bruce slowly started to understand what he was becoming. His irrationality and fear of losing the one he loves had made him lose his own ideals. Psychiatrist Carl Jung explains actions similar to Bruce’s by stating, “It is often tragic to see how blatantly a man bungles his own life and the life of others yet remains totally incapable of seeing how much the whole tragedy originates in himself .” Bruce had become Batman in a quest to use his talents for the well-being of Gotham, but now, he was becoming the very criminal, he swore to defeat. He was so blinded by his obsession with defeating criminals, that he didn’t understand that his ways had not only sparked fear in criminals, but also civilians. He had abused his powers, and instead of trying to save the city, he was trying to help himself. Luckily, Bruce understood the changes within him, and decided to act upon it. In the climactic third act of the trilogy, Bruce finally comes to terms with himself and decides to hang up the mantle of Batman, becoming his own best friend. In the final scenes of The Dark Knight Rises, Bruce has finally released himself of his alter ego of Batman, and retires in France with his new girlfriend, Selina Kyle. After eight years of being Batman, fighting criminals and terrorists, and almost becoming an unhinged criminal himself, Bruce realizes that he must value his own life, as that is the one he can be sure to protect. This theme is also portrayed in “The Journey” by Mary Oliver, as in the final lines of the play, Oliver states that “the only thing you could do-- determined to save the only life you could save”. Bruce had finally understood that he could not be Batman forever, and his alter ego had burdened his life as long as it was there. He was never happy, and simply lived on anger and fear. With Selina Kyle, who was also his accomplice as Catwoman, he finally understood that he must look out for himself and that his own personal life was important too. He had given all he had to the city, and he owed nothing more to the people he had sworn to protect. Bruce’s retirement gave him peace and happiness, sentiments he had long lacked when he was Batman. The death of Rachel Dawes and the retirement of Batman were both instrumental in Bruce Wayne finally becoming his own best friend. Bruce reached his darkest moment when Rachel had died, as he understood what he was truly becoming now and her death was a rude awakening for him to change. Finally after defeating Bane, Bruce is at terms with himself and is clear at his ideals as he finally achieves the peace and happiness, he so desperately craved for when he was Batman. The story of Batman and Bruce Wayne is a story that revolves around choice, and Bruce’s choice of helping the city, was at the cost of his own personal life. He finally is at peace with himself when he makes the choice to retire, as he understands that he had given all he could and he could finally be at peace.\")\n",
    "docx = nlp(docc)\n",
    "\n",
    "#+ dictionary of words and their counts\n",
    "#+ using non-stop words\n",
    "word_frequencies = {}\n",
    "for word in docx:\n",
    "    if word.text not in stopwords:\n",
    "        if word.text not in word_frequencies.keys():\n",
    "            word_frequencies[word.text] = 1\n",
    "        else:\n",
    "            word_frequencies[word.text] += 1\n",
    "#word_frequencies\n",
    "maximum_frequency = max(word_frequencies.values())\n",
    "maximum_frequency\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "    #print(\"hi\")\n",
    "#word_frequencies\n",
    "sentence_list = [sentence for sentence in docx.sents]\n",
    "#sentence_list\n",
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in sent:\n",
    "        #print(\"hi\")\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if(len(sent.text.split(' '))<30):\n",
    "                \n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent.text] = word_frequencies[word.text.lower()]\n",
    "                    #print(word_frequencies[word.text.lower()])\n",
    "                else:\n",
    "                    sentence_scores[sent.text] += word_frequencies[word.text.lower()]\n",
    "                    #print(word_frequencies.text.lower())\n",
    "#sentence_scores\n",
    "from heapq import nlargest\n",
    "summarized_sentences = nlargest(12, sentence_scores, key = sentence_scores.get)\n",
    "final_sentences = [w for w in summarized_sentences]\n",
    "summary = ' '.join(final_sentences)\n",
    "summary\n",
    "#docc\n",
    "#summarized_sentences\n",
    "#q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connotations of words\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "df_yelp = pd.read_table('/Users/rajjanardhan/Downloads/sentiment labelled sentences/yelp_labelled.txt')\n",
    "df_imdb = pd.read_table('/Users/rajjanardhan/Downloads/sentiment labelled sentences/imdb_labelled.txt')\n",
    "df_amz = pd.read_table('/Users/rajjanardhan/Downloads/sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "frames = [df_yelp, df_imdb, df_amz]\n",
    "nlp = spacy.load('en')\n",
    "for colname in frames:\n",
    "    colname.columns = [\"Message\", \"Target\"]\n",
    "#df_yelp.columns\n",
    "keys = [\"Yelp\", \"IMDB\", \"Amazon\"]\n",
    "\n",
    "df = pd.concat(frames, keys = keys)\n",
    "#df.shape\n",
    "#df.head()\n",
    "df.to_csv(\"sentimentdataset1.csv\")\n",
    "df.columns\n",
    "df.isnull().sum()\n",
    "stopwords = list(STOP_WORDS)\n",
    "#stopwords\n",
    "docx = nlp(\"This is a quick test of the usage of Lemma and I am really hoping this is gonna work. Chickens are really cool.\")\n",
    "#for word in docx:\n",
    "     #print(word.text, \"Lemma =>\", word.lemma_)\n",
    "#for word in docx:\n",
    " #   if word.lemma_ != '-PRON-':\n",
    "  #      print(word.lemma_.lower().strip())\n",
    "\n",
    "#[word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word]\n",
    "#for word in docx:\n",
    " \n",
    "    #if word.is_stop == False and not word.is_punct:\n",
    "    #    print(word)\n",
    "[word for word in docx if word.is_stop == False and not word.is_punct]\n",
    "\n",
    "mysents = []\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from spacy.lang.en import English \n",
    "parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in mytokens]\n",
    "    mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations]\n",
    "    \n",
    "    return mytokens\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "classifier = LinearSVC()\n",
    "\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"Message\"]\n",
    "ylabels = df[\"Target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size = 0.2, random_state = 42)\n",
    "pipe = Pipeline([('cleaner', predictors()), ('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "sample_prediction = pipe.predict(X_test)\n",
    "\n",
    "#for(sample, pred) in zip(X_test, sample_prediction):\n",
    " \n",
    "    #print(sample, \"Prediction =>\", pred)\n",
    "\n",
    "#print(\"Accuracy: \", pipe.score(X_test, y_test))\n",
    "#print(\"Accuracy: \", pipe.score(X_test, sample_prediction))\n",
    "\n",
    "pipe.predict([\"This was a great movie\"])\n",
    "\n",
    "print(\"Accuracy: \", pipe.score(X_train, y_train))\n",
    "example = [\"This movie was really pathetic\", \"This movie was beautifully made\", \"The plot to the movie contradicted its ending\"]\n",
    "pipe.predict(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "text = nlp(\"77 y o woman in NAD with a h/o CAD, DM2, asthma and HTN on altace for 8 years awoke from sleep around 2:30 am this morning of a sore throat and swelling of tongue. She came immediately to the ED b/c she was having difficulty swallowing and some trouble breathing due to obstruction caused by the swelling. She has never had a similar reaction ever before and she did not have any associated SOB, chest pain, itching, or nausea. She has not noticed any rashes, and has been afebrile. She says that she feels like it is swollen down in her esophagus as well. In the ED she was given 25mg benadryl IV, 125 mg solumedrol IV and pepcid 20 mg IV. This has helped the swelling some but her throat still hurts and it hurts to swallow. Nothing else was able to relieve the pain and nothing make it worse though she has not tried to drink any fluids because of trouble swallowing. She denies any recent travel, recent exposure to unusual plants or animals or other allergens. She has not started any new medications, has not used any new lotions or perfumes and has not eaten any unusual foods. Patient has not taken any of her oral medications today\")\n",
    "\n",
    "for word in text.ents:\n",
    "    print(word.text, word.label_)\n",
    "displacy.render(text, style = \"ent\", jupyter = True)\n",
    "spacy.explain('NORP')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "thisdoc = nlp(\"77 y o woman in NAD with a h/o CAD, DM2, asthma and HTN on altace for 8 years awoke from sleep around 2:30 am this morning of a sore throat and swelling of tongue. She came immediately to the ED b/c she was having difficulty swallowing and some trouble breathing due to obstruction caused by the swelling. She has never had a similar reaction ever before and she did not have any associated SOB, chest pain, itching, or nausea. She has not noticed any rashes, and has been afebrile. She says that she feels like it is swollen down in her esophagus as well. In the ED she was given 25mg benadryl IV, 125 mg solumedrol IV and pepcid 20 mg IV. This has helped the swelling some but her throat still hurts and it hurts to swallow. Nothing else was able to relieve the pain and nothing make it worse though she has not tried to drink any fluids because of trouble swallowing. She denies any recent travel, recent exposure to unusual plants or animals or other allergens. She has not started any new medications, has not used any new lotions or perfumes and has not eaten any unusual foods. Patient has not taken any of her oral medications today\")\n",
    "\n",
    "for token in thisdoc.ents:\n",
    "    print(token.text, token.start_char, token.end_char, token.label_)\n",
    "    \n",
    "TRAIN_DATA = [\n",
    "    ('77 y o woman in NAD with a h/o CAD, DM2, asthma and HTN on altace for 8 years', {\n",
    "        'entities':[(0, 5, 'AGE')]\n",
    "    }),\n",
    "    ]\n",
    "#})]\n",
    "#)]\n",
    "@plac.annotations(\n",
    "    model = (\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str)\n",
    "    output_dir = (\"C:/Users/rajjanardhan/Documents/Pythonprograms\", \"option\", \"o\", Path)\n",
    "    n_iter = (\"Number of iterations\", \"option\", \"n\", int))\n",
    "\n",
    "\n",
    "model = None\n",
    "n_iter = 100\n",
    "output_dir = Path(\"C:/Users/rajjanardhan/Documents/Pythonprograms\")\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '$'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')\n",
    "    print(\"Created blank 'en' model\")\n",
    "    \n",
    "    \n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_piper('ner')\n",
    "    \n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            nlp.update(\n",
    "                [text],\n",
    "                [annotations],\n",
    "                drop = 0.5,\n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print(losses)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('asdfasdfasdfasdf')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(\"Tensorflow Version: \" + tf.__version__)\n",
    "print(8)\n",
    "\n",
    "corpus = ['king is a smart man', \n",
    "          'queen is a wise woman', \n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong', \n",
    "          'woman is witty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen',\n",
    "            'women is a smart queen',\n",
    "         'prince and princess are smart']\n",
    "newwords= []\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "def remove_stop_words(corpus):\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tp = text.split(' ')\n",
    "        for stop_word in spacy_stopwords:\n",
    "            if stop_word in tp:\n",
    "                tp.remove(stop_word)\n",
    "        results.append(\" \".join(tp))\n",
    "    return results\n",
    "    \n",
    "\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])\n",
    "corpus = remove_stop_words(corpus)\n",
    "            \n",
    "#newwords\n",
    "#text = newwords\n",
    "word2int = {}\n",
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "words = set(words)\n",
    "print(words)\n",
    "\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    \n",
    "sentences = []\n",
    "for sentence in corpus:\n",
    "    sentences.append(sentence.split())\n",
    "    \n",
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx+WINDOW_SIZE, len(sentence))+1]:\n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "                \n",
    "for text in corpus:\n",
    "    print(text)\n",
    "    \n",
    "df = pd.DataFrame(data, columns= ['input', 'label'])\n",
    "\n",
    "df.head(10)\n",
    "df.shape\n",
    "word2int\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for x,y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[x]))\n",
    "    Y.append(to_one_hot_encoding(word2int[y]))\n",
    "\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape = (None, ONE_HOT_DIM))\n",
    "\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1]))\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_layer, W2), b2))\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis = [1]))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "iteration = 20000\n",
    "for i in range(iteration):\n",
    "    sess.run(train_op, feed_dict={x:X_train, y_label: Y_train})\n",
    "    if i % 3000 == 0:\n",
    "        print('iteration ' + str(i) + ' loss is: ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))\n",
    "        \n",
    "vectors = sess.run(W1+b1)\n",
    "#print(vectors)\n",
    "\n",
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "print(w2v_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1, x2))\n",
    "\n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis = 0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis = 0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis = 0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis = 0)[1] + PADDING\n",
    "\n",
    "plt.xlim(x_axis_min, x_axis_max)\n",
    "plt.ylim(y_axis_min, y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "print(\"hji\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import plac\n",
    "from pathlib import Path\n",
    "import random\n",
    "print(2)\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Hello to everyone reading this code. I'm gonna try to make something cool happen.\")\n",
    "cleaned = [y for y in doc if not y.is_stop and y.pos != 'PUNCT']\n",
    "print(\"hi\")\n",
    "raw = [(x.lemma_, x.pos) for x in cleaned]\n",
    "print(raw)\n",
    "\n",
    "\n",
    "def offseter(lbl, doc, matchitem):\n",
    "    o_one = len(str(doc[0:matchitem[1]]))\n",
    "    subdoc = doc[matchitem[1]:matchitem[2]]\n",
    "    o_two = o_one + len(str(subdoc))\n",
    "    return (o_one, o_two, lbl)\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "    \n",
    "label = 'MEDDATADIR'\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "for i in ['local anaesthesia', 'local', 'anaesthesia']:\n",
    "    matcher.add(label, None, nlp(i))\n",
    "    \n",
    "one = nlp(\"On Thursdays, Robert likes to eat BBQ Chicken.\")\n",
    "matches = matcher(one)\n",
    "[match for match in matches]\n",
    "print(11)\n",
    "res = [] \n",
    "to_train_ents = []\n",
    "with open('MedicalNote.txt') as gh:\n",
    "    line = True\n",
    "    print('digital')\n",
    "    while line:\n",
    "        line = gh.readline()\n",
    "        mnlp_line = nlp(line)\n",
    "        matches = matcher(mnlp_line)\n",
    "        res = [offseter(label, mnlp_line, x) for x in matches]\n",
    "        to_train_ents.append((line, dict(entities=res)))\n",
    "print('chicken')\n",
    "optimizer = nlp.begin_training()\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    for itn in range(20):\n",
    "        losses = {}\n",
    "        random.shuffle(to_train_ents)\n",
    "        for item in to_train_ents:\n",
    "            nlp.update([item[0]], [item[1]],sgd=optimizer, drop = 0.35, losses = losses)\n",
    "            \n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(3)\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "#print(2)\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "ner.add_label(\"MEDICAL_SAMPLE\")\n",
    "#print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(u\"I watch cricket.\")\n",
    "entities = [(8, 15, \"SPORT\")]\n",
    "tags = biluo_tags_from_offsets(doc, entities)\n",
    "assert tags == [\"\", \"\", \"\", \"\"]\n",
    "\n",
    "optimizer = nlp.begin_training(get_data)\n",
    "for itn in range(100):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        doc = GoldParse(doc, entities = entity_offsets)\n",
    "        nlp.update([doc], [gold], drop = 0.5, sgd = optimizer)\n",
    "        \n",
    "nlp.to_disk(\"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "print(\"hj\")\n",
    "import csv\n",
    "\n",
    "with open('/Users/rajjanardhan/Documents/Pythonprograms/ner_dataset.csv','r') as csvin, open('/Users/rajjanardhan/Documents/Pythonprograms/NAD.txt', 'w') as tsvout:\n",
    "    csvin = csv.reader(csvin)\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "\n",
    "    for row in csvin:\n",
    "        tsvout.writerow(row)\n",
    "def tsv_to_json_format(input_path, output_path, unknown_label):\n",
    "    try:\n",
    "        f=open(input_path, 'r')\n",
    "        fp = open(output_path, 'w')\n",
    "        data_dict = {}\n",
    "        annotations = []\n",
    "        label_dict = {}\n",
    "        s=\"\"\n",
    "        start = 0\n",
    "        print(2)\n",
    "        for line in f:\n",
    "            \n",
    "            if line[0:len(line)-1]!='.\\t0':\n",
    "                word = line.split('\\t')\n",
    "                print(word)\n",
    "                entity = word\n",
    "                s += str(word) + ' '\n",
    "                entity = entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d = {}\n",
    "                        d['text'] = word\n",
    "                        d['start']=start\n",
    "                        d['end'] = start + len(word)-1\n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity] = []\n",
    "                            label_dict[entity].append(d)\n",
    "                start += len(word) + 1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(len(label_dict[ents]))):\n",
    "                        if(label_dict[ents][i][text]!=''):\n",
    "                            l=[ents,label_dicts[ents][i]]\n",
    "                            for j in range(i+1, len(label_dict[ents][j]['text'])):\n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):\n",
    "                                    di = {}\n",
    "                                    di['start'] = label_dict[ents][j]['start']\n",
    "                                    di['end'] = label_dict[ents][j]['end']\n",
    "                                    di['text'] = label_dict[ents][j]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text'] = ''\n",
    "                            label_list.append(l)\n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label = {}\n",
    "                    label['label'] = [entities[0]]\n",
    "                    label['points'] = entities[1:]\n",
    "                    annotations.append(label)\n",
    "                \n",
    "               # a\n",
    "                data_dict['annotation'] = annotations\n",
    "                annotations = []\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict = {}\n",
    "                start = 0\n",
    "                label_dict = {}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"File is unable to be processed\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "    \n",
    "    \n",
    "tsv_to_json_format('/Users/rajjanardhan/Documents/Pythonprograms/ner_dataset.csv', '/Users/rajjanardhan/Documents/Pythonprograms/ner_corpus_260.json', 'abc')\n",
    "           \n",
    "                \n",
    "print(\"FINSIHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import lagging\n",
    "#import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import plac\n",
    "\n",
    "@plac.annotations(input_file=(\"Input file\", \"option\", \"i\", str), output_file=(\"Output file\", \"option\", \"o\", str))\n",
    "\n",
    "def main(input_file=None, output_file=None):\n",
    "    try:\n",
    "        train_data = []\n",
    "        lines=[]\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "                for label in labels:\n",
    "                    entities.append((point['start'], point['end']+1, label))\n",
    "                    \n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "        \n",
    "        print(training_data)\n",
    "        \n",
    "        with open(output_file, 'wb') as fp:\n",
    "            pickle.dump(training_data, fp)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process\" + input_file + \"\\n\" + \"error =\" + str(e))\n",
    "        return None\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)\n",
    "        \n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "LABEL = \"SPORT\"\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"Basketball is a game that is full of scoring and defense.\",\n",
    "        {\"entities\": [(0, 10, LABEL)]},\n",
    "    ),\n",
    "    (\"Do they start?\", {\"entities\": []}),\n",
    "    (\n",
    "        \"basketball is a game that is full of scoring and defense\",\n",
    "        {\"entities\": [(0, 10, LABEL)]},\n",
    "    ),\n",
    "    (\"basketball is a cruel game that sometimes results in heartbreak\", {\"entities\": [(0, 10, LABEL)]}),\n",
    "    (\n",
    "        \"Lebron James and Kobe Bryant all played basketball\",\n",
    "        {\"entities\": [(42, 52, LABEL)]},\n",
    "    ),\n",
    "    (\"basketball?\", {\"entities\": [(0, 10, LABEL)]}),\n",
    "    (\"superman\", {\"entities\": []}),\n",
    "]\n",
    "\n",
    "print(2)\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "\n",
    "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
    "    \n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  \n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  \n",
    "        print(\"Created blank 'en' model\")\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "   \n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  \n",
    "    \n",
    "    ner.add_label(\"FRUIT\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  \n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size = sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                print(\"Got in the batch annotation section\")\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"Do you like basketball?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        #nlp3 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.begin_training(get_data)\n",
    "for itn in range(100):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab = Vocab(tag_map={\"N\": {\"pos\": \"NOUN\"}, \"V\": {\"pos\": \"VERB\"}})\n",
    "doc = Doc(vocab, words=[\"I\", \"like\", \"stuff\"])\n",
    "gold = GoldParse(doc, tags=[\"N\", \"V\", \"N\"])\n",
    "doc = Doc(Vocab(), words=[\"Facebook\", \"released\", \"React\", \"in\", \"2014\"])\n",
    "gold = GoldParse(doc, entities=[\"U-ORG\", \"O\", \"U-TECHNOLOGY\", \"O\", \"U-DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
